{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML HW1 P3",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larrymoralez/ML-HW-1/blob/master/ML_HW1_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vejT2jnXXuph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#Load the MNIST Data set, split it into training and test sets, and convert it to 2D\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "# if run into problems use (60000, 784, 1)?\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "s = np.random.permutation(len(train_labels))\n",
        "\n",
        "\n",
        "#Define softmax function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum() \n",
        "\n",
        "\n",
        "#Set initial parameters \n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "np.random.seed(42)\n",
        "initial_weight = np.random.randn(10, 784) # needs to be same dimensions as train_images. #for problem 3, do the same thing for 10, 28, 28 matrix (10,784) or (784, 10)\n",
        "weight = initial_weight\n",
        "n = len(train_images)\n",
        "b = np.random.random_sample()\n",
        "\n",
        "\n",
        "#MSE loss function\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(train_images)):\n",
        "        xi = train_images[i]\n",
        "        yi = train_labels[i]\n",
        "        z = xi.T.dot(weight) + b\n",
        "        a = softmax(z)\n",
        "        w_gradient = (a-yi)*(a*(1-a))*xi          #aprime is a*(1-a), so (a-y)aprime, if the index of the test image argmax = test label\n",
        "        weight = weight - lr * w_gradient\n",
        "        b_gradient = (a-yi)*(a*(1-a))\n",
        "        b = b - lr * b_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}