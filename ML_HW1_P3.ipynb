{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML HW1 P3",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larrymoralez/ML-HW-1/blob/master/ML_HW1_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vejT2jnXXuph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "007a74be-be72-4e43-dd23-c518537ed3f2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#Load the MNIST Data set, split it into training and test sets, and convert it to 2D\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "# if run into problems use (60000, 784, 1)?\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "#Define softmax function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum() \n",
        "\n",
        "#Shuffle the training labels\n",
        "s = np.random.permutation(len(train_labels))\n",
        "train_labels[s[i]]\n",
        "\n",
        "#Set initial parameters \n",
        "batch_size = 5000\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "np.random.seed(42)\n",
        "initial_weight = np.random.randn(784, 10) # needs to be same dimensions as train_images. #for problem 3, do the same thing for 10, 28, 28 matrix (10,784) or (784, 10)\n",
        "weight = np.array(initial_weight, dtype=np.float64)\n",
        "n = len(train_images)\n",
        "b = np.random.random_sample(10)\n",
        "b = np.array(b, dtype=np.float64)\n",
        "delta = np.identity(10)\n",
        "gradient_bias = np.zeros(10)\n",
        "gradient_weight = np.zeros((10, 784))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(batch_size):\n",
        "        offset = epoch * batch_size + i\n",
        "        xi = train_images[s[offset]]\n",
        "        yi = train_labels[s[offset]]\n",
        "        z = xi.T.dot(weight) + b\n",
        "        a = softmax(z)\n",
        "        gradient_bias = gradient_bias + yi.dot(delta - a)\n",
        "        new_gradient_weight = np.zeros((10, 784))\n",
        "        for j in range(len(gradient_bias)):\n",
        "            for k in range(len(xi)):\n",
        "                new_gradient_weight[j,k] = gradient_bias[j] * xi [k]\n",
        "        gradient_weight = gradient_weight + new_gradient_weight               \n",
        "\n",
        "\n",
        "final_bias = gradient_bias/(batch_size * epochs)\n",
        "final_weight = gradient_weight/(batch_size * epochs)\n",
        "\n",
        "#Run test images through the model to get predictions       \n",
        "prediction_array = [0 for x in range(len(test_images))]\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    x = test_images[i]\n",
        "    z = final_weight.dot(x) + final_bias\n",
        "    a_predict = softmax(z)\n",
        "    prediction_array[i] = a_predict\n",
        "\n",
        "\n",
        "#Check predictions and calculate accuracy\n",
        "count_correct = 0\n",
        "\n",
        "for i in range(len(test_labels)):\n",
        "    y = test_labels[i]\n",
        "    if np.argmax(prediction_array[i]) == np.argmax(y):\n",
        "        count_correct = count_correct + 1\n",
        "    accuracy = count_correct/(len(test_images))\n",
        "\n",
        "print(count_correct) \n",
        "print(accuracy)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-abc1edd0673c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Shuffle the training labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#Set initial parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r85SV5x19Y_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSQFkbba9VcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}